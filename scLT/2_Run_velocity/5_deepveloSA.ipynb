{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 23:05:19.072097: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-03 23:05:19.072128: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import anndata as ad\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "SEED = 2024\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 3186 × 2000\n",
      "    obs: 'nCount_RNA', 'nFeature_RNA', 'percent.mt', 'nCount_SCT', 'nFeature_SCT', 'sample', 'S.Score', 'G2M.Score', 'Phase', 'integrated_snn_res.0.5', 'seurat_clusters', 'palantir_clusters', 'mono1', 'neu2', 'dc3', 'baso4', 'ery5', 'eos6', 'mep7', 'gmp8', 'cell_type', 'integrated_snn_res.0.4', 'integrated_snn_res.2', 'cell_type2', 'DF_score', 'DF_class', 'orig.lib', 'nCount_spliced', 'nFeature_spliced', 'nCount_unspliced', 'nFeature_unspliced', 'nCount_ambiguous', 'nFeature_ambiguous', 'celltype', 'initial_size_unspliced', 'initial_size_spliced', 'initial_size', 'n_counts', 'velocity_self_transition'\n",
      "    var: 'vst.mean', 'vst.variance', 'vst.variance.expected', 'vst.variance.standardized', 'vst.variable', 'highly_variable_genes', 'gene_count_corr', 'means', 'dispersions', 'dispersions_norm', 'highly_variable', 'velocity_gamma', 'velocity_qreg_ratio', 'velocity_r2', 'velocity_genes'\n",
      "    uns: 'cell1_list', 'cell2_list_exp', 'cell_type2_colors', 'celltype_colors', 'cos_sim_exp_list', 'cos_sim_max_list', 'cos_sim_random_list', 'log1p', 'neighbors', 'sample_colors', 'velocity_graph', 'velocity_graph_neg', 'velocity_params'\n",
      "    obsm: 'X_cce', 'X_pal', 'X_pca', 'X_umap', 'velocity_cce'\n",
      "    layers: 'Ms', 'Mu', 'ambiguous', 'spliced', 'unspliced', 'variance_velocity', 'velocity'\n",
      "    obsp: 'connectivities', 'distances'\n"
     ]
    }
   ],
   "source": [
    "adata = sc.read_h5ad(\"LSK_lineage.h5ad\")\n",
    "print(adata)\n",
    "adata_raw = adata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 23:05:33.925936: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-03 23:05:33.926273: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-06-03 23:05:33.926368: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-06-03 23:05:33.971913: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-06-03 23:05:33.973112: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-06-03 23:05:34.049806: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           128064      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 16)           1040        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 16)           1040        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 16)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 130,144\n",
      "Trainable params: 130,144\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2000)              130000    \n",
      "=================================================================\n",
      "Total params: 131,360\n",
      "Trainable params: 131,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 23:05:34.891809: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1434/1434 [==============================] - 6s 3ms/step - loss: 2.9033 - reconstruction_loss: 0.1196 - kl_loss: 1.2777 - val_loss: 0.2491 - val_reconstruction_loss: 0.1659 - val_kl_loss: 0.0832\n",
      "Epoch 2/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.1867 - reconstruction_loss: 0.1095 - kl_loss: 0.0475 - val_loss: 0.1578 - val_reconstruction_loss: 0.1558 - val_kl_loss: 0.0020\n",
      "Epoch 3/100\n",
      "1434/1434 [==============================] - 6s 4ms/step - loss: 0.1160 - reconstruction_loss: 0.1033 - kl_loss: 0.0102 - val_loss: 0.1513 - val_reconstruction_loss: 0.1511 - val_kl_loss: 1.2106e-04\n",
      "Epoch 4/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.1047 - reconstruction_loss: 0.0993 - kl_loss: 0.0033 - val_loss: 0.1479 - val_reconstruction_loss: 0.1478 - val_kl_loss: 5.2720e-05\n",
      "Epoch 5/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0997 - reconstruction_loss: 0.0969 - kl_loss: 0.0012 - val_loss: 0.1431 - val_reconstruction_loss: 0.1431 - val_kl_loss: 1.0550e-05\n",
      "Epoch 6/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0962 - reconstruction_loss: 0.0954 - kl_loss: 3.9063e-04 - val_loss: 0.1439 - val_reconstruction_loss: 0.1439 - val_kl_loss: 9.5367e-07\n",
      "Epoch 7/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0943 - reconstruction_loss: 0.0942 - kl_loss: 1.1806e-04 - val_loss: 0.1404 - val_reconstruction_loss: 0.1404 - val_kl_loss: 2.3842e-07\n",
      "Epoch 8/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0923 - reconstruction_loss: 0.0932 - kl_loss: 3.1508e-05 - val_loss: 0.1390 - val_reconstruction_loss: 0.1390 - val_kl_loss: 2.6822e-07\n",
      "Epoch 9/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0925 - reconstruction_loss: 0.0925 - kl_loss: 7.8298e-06 - val_loss: 0.1368 - val_reconstruction_loss: 0.1368 - val_kl_loss: 1.7881e-07\n",
      "Epoch 10/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0915 - reconstruction_loss: 0.0920 - kl_loss: 8.9022e-07 - val_loss: 0.1425 - val_reconstruction_loss: 0.1425 - val_kl_loss: 2.3842e-07\n",
      "Epoch 11/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0919 - reconstruction_loss: 0.0917 - kl_loss: 1.1996e-07 - val_loss: 0.1405 - val_reconstruction_loss: 0.1405 - val_kl_loss: 5.9605e-08\n",
      "Epoch 12/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0919 - reconstruction_loss: 0.0913 - kl_loss: 5.8919e-08 - val_loss: 0.1351 - val_reconstruction_loss: 0.1351 - val_kl_loss: 2.9802e-08\n",
      "Epoch 13/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0913 - reconstruction_loss: 0.0912 - kl_loss: 4.6096e-08 - val_loss: 0.1377 - val_reconstruction_loss: 0.1377 - val_kl_loss: 2.9802e-08\n",
      "Epoch 14/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0914 - reconstruction_loss: 0.0909 - kl_loss: 2.7724e-08 - val_loss: 0.1362 - val_reconstruction_loss: 0.1362 - val_kl_loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0912 - reconstruction_loss: 0.0909 - kl_loss: 1.0620e-08 - val_loss: 0.1377 - val_reconstruction_loss: 0.1377 - val_kl_loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0901 - reconstruction_loss: 0.0909 - kl_loss: 1.2615e-08 - val_loss: 0.1363 - val_reconstruction_loss: 0.1363 - val_kl_loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1434/1434 [==============================] - 6s 4ms/step - loss: 0.0912 - reconstruction_loss: 0.0907 - kl_loss: 5.7568e-09 - val_loss: 0.1355 - val_reconstruction_loss: 0.1355 - val_kl_loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0914 - reconstruction_loss: 0.0906 - kl_loss: 7.2947e-09 - val_loss: 0.1367 - val_reconstruction_loss: 0.1367 - val_kl_loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1434/1434 [==============================] - 6s 4ms/step - loss: 0.0893 - reconstruction_loss: 0.0906 - kl_loss: 7.0245e-09 - val_loss: 0.1348 - val_reconstruction_loss: 0.1348 - val_kl_loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0905 - reconstruction_loss: 0.0905 - kl_loss: 1.2054e-09 - val_loss: 0.1353 - val_reconstruction_loss: 0.1353 - val_kl_loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0909 - reconstruction_loss: 0.0905 - kl_loss: 1.6003e-09 - val_loss: 0.1367 - val_reconstruction_loss: 0.1367 - val_kl_loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0905 - reconstruction_loss: 0.0905 - kl_loss: 1.8497e-09 - val_loss: 0.1365 - val_reconstruction_loss: 0.1365 - val_kl_loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0911 - reconstruction_loss: 0.0904 - kl_loss: 2.2861e-10 - val_loss: 0.1362 - val_reconstruction_loss: 0.1362 - val_kl_loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0907 - reconstruction_loss: 0.0904 - kl_loss: 4.3228e-09 - val_loss: 0.1361 - val_reconstruction_loss: 0.1361 - val_kl_loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0913 - reconstruction_loss: 0.0903 - kl_loss: 2.0575e-09 - val_loss: 0.1391 - val_reconstruction_loss: 0.1391 - val_kl_loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0906 - reconstruction_loss: 0.0904 - kl_loss: 1.4340e-09 - val_loss: 0.1369 - val_reconstruction_loss: 0.1369 - val_kl_loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0895 - reconstruction_loss: 0.0903 - kl_loss: 5.6113e-10 - val_loss: 0.1358 - val_reconstruction_loss: 0.1358 - val_kl_loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0900 - reconstruction_loss: 0.0902 - kl_loss: 2.0159e-09 - val_loss: 0.1362 - val_reconstruction_loss: 0.1362 - val_kl_loss: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0909 - reconstruction_loss: 0.0902 - kl_loss: 4.1565e-11 - val_loss: 0.1368 - val_reconstruction_loss: 0.1368 - val_kl_loss: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0897 - reconstruction_loss: 0.0902 - kl_loss: 1.2470e-09 - val_loss: 0.1358 - val_reconstruction_loss: 0.1358 - val_kl_loss: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0896 - reconstruction_loss: 0.0901 - kl_loss: 2.4731e-09 - val_loss: 0.1369 - val_reconstruction_loss: 0.1369 - val_kl_loss: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0900 - reconstruction_loss: 0.0901 - kl_loss: 1.9536e-09 - val_loss: 0.1368 - val_reconstruction_loss: 0.1368 - val_kl_loss: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0900 - reconstruction_loss: 0.0901 - kl_loss: 2.7017e-10 - val_loss: 0.1349 - val_reconstruction_loss: 0.1349 - val_kl_loss: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0894 - reconstruction_loss: 0.0901 - kl_loss: 1.0807e-09 - val_loss: 0.1365 - val_reconstruction_loss: 0.1365 - val_kl_loss: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0885 - reconstruction_loss: 0.0901 - kl_loss: 5.6113e-10 - val_loss: 0.1361 - val_reconstruction_loss: 0.1361 - val_kl_loss: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0910 - reconstruction_loss: 0.0900 - kl_loss: 1.6003e-09 - val_loss: 0.1362 - val_reconstruction_loss: 0.1362 - val_kl_loss: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0902 - reconstruction_loss: 0.0901 - kl_loss: 0.0000e+00 - val_loss: 0.1373 - val_reconstruction_loss: 0.1373 - val_kl_loss: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0909 - reconstruction_loss: 0.0901 - kl_loss: 3.1174e-10 - val_loss: 0.1355 - val_reconstruction_loss: 0.1355 - val_kl_loss: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0899 - reconstruction_loss: 0.0900 - kl_loss: 8.3131e-10 - val_loss: 0.1360 - val_reconstruction_loss: 0.1360 - val_kl_loss: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0921 - reconstruction_loss: 0.0901 - kl_loss: 1.6626e-10 - val_loss: 0.1360 - val_reconstruction_loss: 0.1360 - val_kl_loss: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0915 - reconstruction_loss: 0.0900 - kl_loss: 1.2262e-09 - val_loss: 0.1362 - val_reconstruction_loss: 0.1362 - val_kl_loss: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0922 - reconstruction_loss: 0.0900 - kl_loss: 3.9487e-10 - val_loss: 0.1376 - val_reconstruction_loss: 0.1376 - val_kl_loss: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0892 - reconstruction_loss: 0.0900 - kl_loss: 1.2470e-10 - val_loss: 0.1375 - val_reconstruction_loss: 0.1375 - val_kl_loss: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0902 - reconstruction_loss: 0.0899 - kl_loss: 0.0000e+00 - val_loss: 0.1368 - val_reconstruction_loss: 0.1368 - val_kl_loss: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0903 - reconstruction_loss: 0.0900 - kl_loss: 0.0000e+00 - val_loss: 0.1369 - val_reconstruction_loss: 0.1369 - val_kl_loss: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0899 - reconstruction_loss: 0.0899 - kl_loss: 2.7017e-10 - val_loss: 0.1366 - val_reconstruction_loss: 0.1366 - val_kl_loss: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0885 - reconstruction_loss: 0.0899 - kl_loss: 9.1444e-10 - val_loss: 0.1379 - val_reconstruction_loss: 0.1379 - val_kl_loss: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0905 - reconstruction_loss: 0.0899 - kl_loss: 1.6626e-10 - val_loss: 0.1366 - val_reconstruction_loss: 0.1366 - val_kl_loss: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0900 - reconstruction_loss: 0.0899 - kl_loss: 1.2470e-10 - val_loss: 0.1362 - val_reconstruction_loss: 0.1362 - val_kl_loss: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0903 - reconstruction_loss: 0.0899 - kl_loss: 0.0000e+00 - val_loss: 0.1366 - val_reconstruction_loss: 0.1366 - val_kl_loss: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0901 - reconstruction_loss: 0.0899 - kl_loss: 0.0000e+00 - val_loss: 0.1360 - val_reconstruction_loss: 0.1360 - val_kl_loss: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0898 - reconstruction_loss: 0.0899 - kl_loss: 3.1174e-10 - val_loss: 0.1369 - val_reconstruction_loss: 0.1369 - val_kl_loss: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0896 - reconstruction_loss: 0.0899 - kl_loss: 0.0000e+00 - val_loss: 0.1356 - val_reconstruction_loss: 0.1356 - val_kl_loss: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0904 - reconstruction_loss: 0.0899 - kl_loss: 0.0000e+00 - val_loss: 0.1352 - val_reconstruction_loss: 0.1352 - val_kl_loss: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0895 - reconstruction_loss: 0.0899 - kl_loss: 0.0000e+00 - val_loss: 0.1352 - val_reconstruction_loss: 0.1352 - val_kl_loss: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0907 - reconstruction_loss: 0.0898 - kl_loss: 0.0000e+00 - val_loss: 0.1357 - val_reconstruction_loss: 0.1357 - val_kl_loss: 0.0000e+00\n",
      "Epoch 57/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0903 - reconstruction_loss: 0.0898 - kl_loss: 2.9096e-10 - val_loss: 0.1380 - val_reconstruction_loss: 0.1380 - val_kl_loss: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0883 - reconstruction_loss: 0.0898 - kl_loss: 2.9096e-10 - val_loss: 0.1360 - val_reconstruction_loss: 0.1360 - val_kl_loss: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0890 - reconstruction_loss: 0.0898 - kl_loss: 0.0000e+00 - val_loss: 0.1361 - val_reconstruction_loss: 0.1361 - val_kl_loss: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0892 - reconstruction_loss: 0.0898 - kl_loss: 0.0000e+00 - val_loss: 0.1357 - val_reconstruction_loss: 0.1357 - val_kl_loss: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0901 - reconstruction_loss: 0.0898 - kl_loss: 5.8191e-10 - val_loss: 0.1356 - val_reconstruction_loss: 0.1356 - val_kl_loss: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0908 - reconstruction_loss: 0.0898 - kl_loss: 0.0000e+00 - val_loss: 0.1368 - val_reconstruction_loss: 0.1368 - val_kl_loss: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0893 - reconstruction_loss: 0.0897 - kl_loss: 2.9096e-10 - val_loss: 0.1363 - val_reconstruction_loss: 0.1363 - val_kl_loss: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0904 - reconstruction_loss: 0.0897 - kl_loss: 3.9487e-10 - val_loss: 0.1355 - val_reconstruction_loss: 0.1355 - val_kl_loss: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0899 - reconstruction_loss: 0.0898 - kl_loss: 4.1565e-10 - val_loss: 0.1364 - val_reconstruction_loss: 0.1364 - val_kl_loss: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0910 - reconstruction_loss: 0.0898 - kl_loss: 0.0000e+00 - val_loss: 0.1361 - val_reconstruction_loss: 0.1361 - val_kl_loss: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0905 - reconstruction_loss: 0.0898 - kl_loss: 0.0000e+00 - val_loss: 0.1363 - val_reconstruction_loss: 0.1363 - val_kl_loss: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0921 - reconstruction_loss: 0.0898 - kl_loss: 4.5722e-10 - val_loss: 0.1354 - val_reconstruction_loss: 0.1354 - val_kl_loss: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0902 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1366 - val_reconstruction_loss: 0.1366 - val_kl_loss: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0876 - reconstruction_loss: 0.0897 - kl_loss: 2.4939e-10 - val_loss: 0.1358 - val_reconstruction_loss: 0.1358 - val_kl_loss: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0892 - reconstruction_loss: 0.0898 - kl_loss: 0.0000e+00 - val_loss: 0.1354 - val_reconstruction_loss: 0.1354 - val_kl_loss: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0902 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1363 - val_reconstruction_loss: 0.1363 - val_kl_loss: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0901 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1374 - val_reconstruction_loss: 0.1374 - val_kl_loss: 0.0000e+00\n",
      "Epoch 74/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0904 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1367 - val_reconstruction_loss: 0.1367 - val_kl_loss: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0886 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1361 - val_reconstruction_loss: 0.1361 - val_kl_loss: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0893 - reconstruction_loss: 0.0897 - kl_loss: 6.2348e-11 - val_loss: 0.1359 - val_reconstruction_loss: 0.1359 - val_kl_loss: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0897 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1364 - val_reconstruction_loss: 0.1364 - val_kl_loss: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0894 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1361 - val_reconstruction_loss: 0.1361 - val_kl_loss: 0.0000e+00\n",
      "Epoch 79/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0905 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1359 - val_reconstruction_loss: 0.1359 - val_kl_loss: 0.0000e+00\n",
      "Epoch 80/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0897 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1357 - val_reconstruction_loss: 0.1357 - val_kl_loss: 0.0000e+00\n",
      "Epoch 81/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0902 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1356 - val_reconstruction_loss: 0.1356 - val_kl_loss: 0.0000e+00\n",
      "Epoch 82/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0897 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1361 - val_reconstruction_loss: 0.1361 - val_kl_loss: 0.0000e+00\n",
      "Epoch 83/100\n",
      "1434/1434 [==============================] - 5s 4ms/step - loss: 0.0902 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1362 - val_reconstruction_loss: 0.1362 - val_kl_loss: 0.0000e+00\n",
      "Epoch 84/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0897 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1372 - val_reconstruction_loss: 0.1372 - val_kl_loss: 0.0000e+00\n",
      "Epoch 85/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0891 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1360 - val_reconstruction_loss: 0.1360 - val_kl_loss: 0.0000e+00\n",
      "Epoch 86/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0907 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1368 - val_reconstruction_loss: 0.1368 - val_kl_loss: 0.0000e+00\n",
      "Epoch 87/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0896 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1377 - val_reconstruction_loss: 0.1377 - val_kl_loss: 0.0000e+00\n",
      "Epoch 88/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0894 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1357 - val_reconstruction_loss: 0.1357 - val_kl_loss: 0.0000e+00\n",
      "Epoch 89/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0906 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1371 - val_reconstruction_loss: 0.1371 - val_kl_loss: 0.0000e+00\n",
      "Epoch 90/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0891 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1363 - val_reconstruction_loss: 0.1363 - val_kl_loss: 0.0000e+00\n",
      "Epoch 91/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0886 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1357 - val_reconstruction_loss: 0.1357 - val_kl_loss: 0.0000e+00\n",
      "Epoch 92/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0898 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1374 - val_reconstruction_loss: 0.1374 - val_kl_loss: 0.0000e+00\n",
      "Epoch 93/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0895 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1369 - val_reconstruction_loss: 0.1369 - val_kl_loss: 0.0000e+00\n",
      "Epoch 94/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0885 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1357 - val_reconstruction_loss: 0.1357 - val_kl_loss: 0.0000e+00\n",
      "Epoch 95/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0907 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1359 - val_reconstruction_loss: 0.1359 - val_kl_loss: 0.0000e+00\n",
      "Epoch 96/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0896 - reconstruction_loss: 0.0897 - kl_loss: 0.0000e+00 - val_loss: 0.1365 - val_reconstruction_loss: 0.1365 - val_kl_loss: 0.0000e+00\n",
      "Epoch 97/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0898 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1356 - val_reconstruction_loss: 0.1356 - val_kl_loss: 0.0000e+00\n",
      "Epoch 98/100\n",
      "1434/1434 [==============================] - 5s 3ms/step - loss: 0.0891 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1358 - val_reconstruction_loss: 0.1358 - val_kl_loss: 0.0000e+00\n",
      "Epoch 99/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0892 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1360 - val_reconstruction_loss: 0.1360 - val_kl_loss: 0.0000e+00\n",
      "Epoch 100/100\n",
      "1434/1434 [==============================] - 4s 3ms/step - loss: 0.0912 - reconstruction_loss: 0.0896 - kl_loss: 0.0000e+00 - val_loss: 0.1358 - val_reconstruction_loss: 0.1358 - val_kl_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f97700830d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var[\"velocity_genes\"] = True\n",
    "\n",
    "# Add noise to data\n",
    "X = np.tile(adata_raw.X.A[:, adata.var[\"velocity_genes\"]], (5, 1))\n",
    "Y = np.tile(adata.layers[\"velocity\"][:, adata.var[\"velocity_genes\"]], (5, 1))\n",
    "noise_sigma = (adata_raw.X.A.std()/70)**2\n",
    "X[adata_raw.shape[0]:, :] += \\\n",
    "    np.random.normal(0, noise_sigma, X[adata_raw.shape[0]:, :].shape)\n",
    "\n",
    "XYpath = \"DeepVelo_SA_prepropcessed.npz\"\n",
    "np.savez(XYpath, X, Y)\n",
    "\n",
    "X = np.load(XYpath)[\"arr_0\"]\n",
    "Y = np.load(XYpath)[\"arr_1\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, \n",
    "                                                    Y, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "#from vae import create_encoder, create_decoder, VAE\n",
    "tf.config.list_physical_devices('GPU') \n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the encoder\n",
    "\"\"\"\n",
    "\n",
    "def create_encoder(input_size = 1000, latent_dim = 16, verbose=1):\n",
    "    encoder_inputs = keras.Input(shape=(input_size,))\n",
    "    x = layers.Dense(64, activation=\"relu\", activity_regularizer=keras.regularizers.l1(1e-6))(encoder_inputs)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\", activity_regularizer=keras.regularizers.l1(1e-6))(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\", activity_regularizer=keras.regularizers.l1(1e-6))(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    if verbose == 1:\n",
    "        encoder.summary()\n",
    "    return encoder\n",
    "\n",
    "\"\"\"\n",
    "## Build the decoder\n",
    "\"\"\"\n",
    "\n",
    "def create_decoder(output_size = 1000, latent_dim = 16, verbose=1):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(16, activation=\"relu\", activity_regularizer=keras.regularizers.l1(1e-6))(latent_inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\", activity_regularizer=keras.regularizers.l1(1e-6))(x)\n",
    "    decoder_outputs = layers.Dense(output_size)(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    if verbose == 1:\n",
    "        decoder.summary()\n",
    "    return decoder\n",
    "\n",
    "\"\"\"\n",
    "## Define the VAE as a `Model` with a custom `train_step`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                    keras.losses.MSE(y, reconstruction)\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def call(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            keras.losses.MSE(y, reconstruction)\n",
    "        )\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "\n",
    "encoder = create_encoder(X.shape[1])\n",
    "decoder = create_decoder(X.shape[1])\n",
    "\n",
    "autoencoder = VAE(encoder, decoder)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.00001)\n",
    "autoencoder.compile(optimizer=opt)\n",
    "\n",
    "autoencoder.fit(X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=10,\n",
    "        shuffle=True,\n",
    "        validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3186, 2000)\n"
     ]
    }
   ],
   "source": [
    "X = adata_raw.X.A[:, adata.var[\"velocity_genes\"]]\n",
    "velocity_deepvelo = autoencoder.predict(X)\n",
    "print(velocity_deepvelo.shape)\n",
    "adata.layers['velocity_dv'] = velocity_deepvelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad(\"adata/DeepVelo_SA.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepveloSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
